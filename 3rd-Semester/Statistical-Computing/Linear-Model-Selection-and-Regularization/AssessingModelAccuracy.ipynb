{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ca80da-d93c-49e6-8e3e-cd2aac4b0742",
   "metadata": {},
   "source": [
    "# Assessing Model Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d2423-04cb-4d61-93c3-6591ab95667e",
   "metadata": {},
   "source": [
    "## Measuring the Quality of the Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb056d8-d82e-40da-aef2-01445e66af2f",
   "metadata": {},
   "source": [
    "### Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff4531-fe68-4b8e-b4d9-29227e4d8202",
   "metadata": {},
   "source": [
    "To evaluate the performance of a statistical learning model we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.\n",
    "\n",
    "In **regression**, the most commonly-used measure is the *mean squared error* (MSE)\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i = 1}^{n} (y_i - \\hat{f}(x_i))^2\n",
    "$$\n",
    "\n",
    "where $\\hat{f}(x_i)$ is the prediction that $\\hat{f}$ gives for the $i$th observation. The MSE will be small if the predicted values are very close to the true responses, and will be larger if for some observations the predicted and true responses differ significantly. \n",
    "\n",
    "In most cases we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. In other words, we want to know whether $\\hat{f}(x_0)$ is approximately equal to $y_0$, where ($x_0, y_0$) is a previously **unseen test observation not used to train the statistical learning method**. We want to choose the method that gives the **lowest test MSE**\n",
    "\n",
    "$$\n",
    "Avg(y_0 - \\hat{f}(x_0))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25673fbb-3103-46a1-89e3-ffded78b4c32",
   "metadata": {},
   "source": [
    "Since the training MSE and the test MSE appear to be related one might imagine simple select a statistical learning method that minimizes the training MSE. However there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a4069-16b1-42ac-a2a0-7f3e582fa6b0",
   "metadata": {},
   "source": [
    "As model flexibility increases (i.e. the ability of a model to fit a wide variety of patterns in the data), training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be **overfitting** the data. This happends because our statistical learning method becomes more capable of fitting even the smallest details in the training data, **including noise and random fluctuations** that don't represent the true underlying patterns. When we overfit the training data, the test MSE will be vary large because the supposed patterns that the method found in the training data simply don't exist in the test data. \n",
    "\n",
    "**N o t e**\\\n",
    "Regardless of whether or not overfitting has ocurred, we almost always expect the training MSE to be smaller than the test MSE becuase most statistical learning methods either directly or indirectly seek to minimize the training MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a6afe-456a-460b-b598-c9c59f69a468",
   "metadata": {},
   "source": [
    "### Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63774d31-1e2c-4f53-8059-0a3f5bf3d955",
   "metadata": {},
   "source": [
    "The expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities:\n",
    "\n",
    "$$\n",
    "E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\n",
    "$$\n",
    "\n",
    "where $E(y_0 - \\hat{f}(x_0))^2$ defines the expected test MSE at $x_0$, and refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets and testes each at $x_0$. The overall expected test MSE can be computed by averaging $E(y_0 - \\hat{f}(x_0))^2 $ over all possible values of $x_0$ in the test set.\n",
    "\n",
    "The equation above tell us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves **low variance** and **low bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a590c5-043b-4122-be4b-7e891397d5fe",
   "metadata": {},
   "source": [
    "#### What do we Mean by *Variance* and *Bias* of a Statistical Learning Method? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21276e6-417d-4837-a664-c0afb4c9214e",
   "metadata": {},
   "source": [
    "*Variance* refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\\hat{f}$. However the estimate $f$ shouldn't vary too much between training sets. If a method has high variance then small changes in the training data can result in large changes in $\\hat{f}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803cf8d-0685-41cd-a9f1-e5b2b6a75917",
   "metadata": {},
   "source": [
    "*Bias* refers to the error that is introduces by approximating a real-life problem. \n",
    "\n",
    "**N o t e**\\\n",
    "More flexible statistical methods have higher variance while having less bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf83174-3fad-4f08-b056-d135015e8f1e",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd62c04-fd81-4e8d-8380-7d755c1a76ec",
   "metadata": {},
   "source": [
    "Consider the following training set $\\{(x_1, y_1), ..., (x_n, y_n)\\}$ where $y_1, ..., y_n$ are qualitative. The approach for quantifying the accuracy of our estimate $\\hat{f}$ is the training *error rate*, the proportion of mistakes that are made if we apply our estimate $\\hat{f}$ to the training observations\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i = 1}^{n} I(y_i \\neq \\hat{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a48f97-890f-48ed-9afe-74149d0c23d5",
   "metadata": {},
   "source": [
    "where $\\hat{y}_i$ is the predicted class label for the $i$th observation using $\\hat{f}$. And $I(y_i \\neq \\hat{y}_i)$ is an **indicator variable** that equals $1$ if $y_i \\neq \\hat{y}_i$ and $0$ if $y_i = \\hat{y}_i$. Thus if $I(y_i \\neq \\hat{y}_i) = 0$ then the $i$th observation was classified correctly by our classification method, otherwise it was misclassified.\n",
    "\n",
    "The eq. above is called *training error* rate because it is computed based on the data that was used to train our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd01db-5857-47f9-a984-b36602b1165e",
   "metadata": {},
   "source": [
    "The *test error* rate associated with a set of test observations of the form $(x_0, y_0)$ is given by\n",
    "\n",
    "$$\n",
    "Avg(I(y_0 \\neq \\hat{y}_0))\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_0$ is the predicted class label that results from applying the classifier to the test observation with predictor $x_0$. A good classifier is one for which the test eror is smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7944a6bf-fcb3-4406-8cb6-4f1bc070f345",
   "metadata": {},
   "source": [
    "Retrieved from: \n",
    "\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An introduction to statistical learning with applications in R* (2nd ed.). Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a0a6a-61c6-490d-992c-1b232beefd7e",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "MSc Statistical Computing by Mathematics Research Center (CIMAT Monterrey)\n",
    "\n",
    "October 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 4.4.1",
   "language": "R",
   "name": "ir44"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
