{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfe41b1-924b-4c42-86cc-be93d70d8fba",
   "metadata": {},
   "source": [
    "# Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cbff8-6f8c-491e-b4e0-5e1b9d58eaa9",
   "metadata": {},
   "source": [
    "The main idea behind the resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model that would not be available from fitting the model only once using the original training sample.\n",
    "\n",
    "Resampling methods can be computationally expensive, because they involve **fitting the same statistical method multiple times using different subsets of the training data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac301cca-18d2-48ca-9419-63715184fa2a",
   "metadata": {},
   "source": [
    "## Cross-Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b7025-7a5a-4e94-a31e-b712a3e95b47",
   "metadata": {},
   "source": [
    "### Validation Set Approach\n",
    "\n",
    "Suppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. It involves randomly dividing the available set of observations into two parts, a *training set* and a *validation set* or *hold-out-set*. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (MSE in the case of a quantitative response), provides an estimate of the test error rate.\n",
    "\n",
    "This approach is simple and easy to implement but it has two potential drawbacks:\n",
    "\n",
    "- The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n",
    "- Only a subset of the observations are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9c77e-99aa-4747-a6e3-de3150f3804e",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "Like validation set approach, leave-one-out cross validation (LOOCV), involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $(x_1, y_1)$ is used for the validation set, and the remaining observations $\\{(x_2, y_2,), ..., (x_n, y_n)\\}$ make up the training set. The statistical learning method is fit on the $n - 1$ training observations, and a prediction $\\hat{y}_1$ is made for the excluded observation using its value $x_1$. Since $(x_1, y_1)$ was not used in the fitting process, $MSE_1 = (y_1, \\hat{y}_1)^2$ provides an approximately unbiased estiamte for the test error. But even though is an unbiased estimate, it's a poor estimate because it's highly variable, since it's based upon a single observation $(x_1, y_1)$. \n",
    "\n",
    "We can repeat the procedure by selecting $(x_2, y_2)$ for the validation data, training the statistical learning procedure on the $n - 1$ observations and computing $MSE_2 = (y_2, \\hat{y}_2)^2$. Repeating this approach $n$ times produces $n$ squared errors, $MSE_1, ..., MSE_n$. The LOOCV estimate for the test MSE is the average of these $n$ test error estimates\n",
    "\n",
    "$$\n",
    "CV_{(n)} = \\frac{1}{n} \\sum_{i = 1}^{n} MSE_i \n",
    "$$\n",
    "\n",
    "LOOCV has a couple of advantages over validation set approach. First, it has far less bias. Second, in contrast to the validation set approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results, i.e. there is no randomness in the training/validation set splits.\n",
    "\n",
    "LOOCV has the potential to be expensive to implement, since the model has to be fit $n$ times. This can be very time consuming if $n$ is large, and if each individual model is slow to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf352b-47eb-4489-b83e-d840635b4513",
   "metadata": {},
   "source": [
    "### k-Fold Cross-Validation\n",
    "\n",
    "An alternative to LOOCV is *k-fold CV*. This approach involves randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit in the remaining $k - 1$ folds. The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. This procedure is repeated $k$ times, each time, a different group of observations is treated as a validation set. This process results in $k$ estimates of the test error, $MSE_1, MSE_2, ..., MSE_k$. The $k$-fold CV estimate is comoputed by averaging these values\n",
    "\n",
    "$$\n",
    "CV_{k} = \\frac{1}{k} \\sum_{i = 1}^{k} MSE_i\n",
    "$$\n",
    "\n",
    "Note that LOOCV is a special case of $k$-fold CV in which $k$ is set to equal to $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fc258-2075-40e9-a423-99355675ef51",
   "metadata": {},
   "source": [
    "### Cross-Validation on Classification Problems\n",
    "\n",
    "Cross-Validation can also be a very useful approach in the classification setting when $Y$ is qualitative. In this setting, cross-validation works just as described earlier, except that rather that using MSE to quantify test error, we instead use the number of misclassified observations. For instance, the LOOCV error rate takes the form\n",
    "\n",
    "$$\n",
    "CV_{n} = \\frac{1}{n}\\sum_{i = 1}^{n} Err_i\n",
    "$$\n",
    "\n",
    "$\\qquad$ where $Err_i = I(y_i \\neq \\hat{y}_i)$. \n",
    "\n",
    "The k-fold CV error rate and validation ser error rates are defined analogously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8132f1-16b2-458f-843d-46aa9f21f025",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "MSc Statistical Computing by Mathematics Research Center (CIMAT Monterrey)\n",
    "\n",
    "October 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 4.4.1",
   "language": "R",
   "name": "ir44"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
